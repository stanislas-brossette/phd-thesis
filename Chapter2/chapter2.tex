%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Numerical Optimization: Introduction}
\label{chapter:optimization}

\nomenclature[x-R]{$\mathbb{R}$}{The Real space}
\nomenclature[x-M]{$\mathcal{M}$}{A Manifold}
\nomenclature[x-T]{$T_x\mathcal{M}$}{The tangent space of manifold $\mathcal{M}$ at point x}
\nomenclature[a-F]{$F$}{The set of linearized feasible directions}
\nomenclature[a-f]{$f$}{Objective function}
\nomenclature[a-c]{$c_i$}{Constraint function}
\nomenclature[a-x]{$x$}{Optimization variable}
\nomenclature[a-xstar]{$x^*$}{Solution of the optimization problem}
\nomenclature[a-E]{${E}$}{Set of index for which constraints are equality constraints}
\nomenclature[a-I]{${I}$}{Set of index for which constraints are inequality constraints}
\nomenclature[x-L]{$\mathcal{L}(x,\lambda)$}{Lagrangian function of the optimization problem}
\nomenclature[G-o]{$\Omega$}{Feasible set}
\nomenclature[z-KKT]{KKT}{Karush-Kuhn-Tucker first order optimality conditions}
\nomenclature[z-LICQ]{LICQ}{Linear Independence Constraints Qualification}
\nomenclature[z-QP]{QP}{Quadratic Programming}
\nomenclature[z-IQP]{IQP}{Inequality constrained Quadratic Programming}

\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi

\section{List of contributions}
\begin{itemize}
  \item Introduction to optimization
  \item Notions of unconstrained optimization
  \item Notions of Constrained Optimization
  \item KKT
  \item Line Search
  \item Trust Region
  \item Filter
  \item Merit function
  \item SQP
  \item Restoration phase
  \item Hessian Approximation: BFGS, SR1
\end{itemize}

%********************************** %First Section  **************************************
\section{Introduction}
In modern science, optimization has a very important place. Mechanical engineers
optimize the shape of stuctural parts. Investors optimize the profit of a
portfolio while minimizing the risks. Chemists optimize the efficiency and speed
of reactions. When it comes to robotics, optimization is everywhere. From the
design of a robot to its actuation. Any positioning of a robot requires to
compute the articular parameters of each joint of the robot, finding such
parameters might be possible by using analytical methods for very simple robots,
but for robots as complex as humanoid robots, it is definitely not possible. And
most often, an optimization process is used.

The goal of an optimization algorithm is to find an optimal solution to a
problem. Optimal in the sense that the solution is an optimum of a given
objective function. And solution of a problem in the sense that is satisfies a
set of constraints defined by the problem. Both the constraints and the
objective function are defined on the variable space, which is the space in
which we search a solution, the space in which the variable lives.

In this chapter, we will use the following notations:
\begin{itemize}
  \item $\mathcal{S}$ is the variable space (usually $\mathbb{R}^n$)
  \item $x\in\mathcal{S}$ is the vector of variables
  \item $f:\mathcal{S}\rightarrow\mathbb{R}$ is the objective function, or Cost function
  \item $c_i:\mathcal{S}\rightarrow\mathbb{R}$ is the i-th constraint function
    ($0\leq i \leq m$)
\end{itemize}

Using these notations, an optimization problem can be written as follows:
\begin{align}
  \min_{\bf x \in \mathcal{S}} & \quad \mbox{\emph{f}}({\bf x}) \nonumber\\
\text{s.t.}&
\left\{
\begin{array}{lr}
c_i = 0 \ \ \ \forall i\in \mathcal{E} \\
c_i \leq 0 \ \ \ \forall i\in \mathcal{I} 
\end{array}\right.
\label{eq:basicOptim}
\end{align}

We call $\mathcal{E}$ the set of index for which the constraints are equality
conditions and $\mathcal{I}$ the set for which the constraints are inequality
conditions.

This set of equations presents the optimisation process under a very general
form. In order to  present the principles of optimization, we will consider
simpler problems. The first type of problem that we will talk about in this
section is the unconstrained problem, that has the particularity to not have any
constraint and its variable space is $\mathbb{R}^n$. Then we will considere the
same problem, but with added constraints, and we will particularly detail one
specific constrained problem optimization algorithm that is called the
Sequential Quadratic Program(SQP). And finally we will study the implications of
solving an optimization problem on a manifold that is different from
$\mathbb{R}^n$.

In this section we will present the theory of optimization methods, starting from
the "simplest" unconstrained optimization and work our way up to more complex
thing by adding constraints and solving problems on non-Euclidian spaces.


\section{Unconstrained Optimization}

An unconstrained optimization problem is a problem that has an objective
function but no constraints. It can be formulated as follows:

\begin{align}
  \min_{\bf x \in \mathcal{S}} & \quad \mbox{\emph{f}}({\bf x})
\label{eq:unconstrainedOptim}
\end{align}

In order to solve this problem, we want to design an algorithm that, starting
from an initial guess $x_0$, will converge toward the solution $x^*$. The
objective function is not necessarily completely known. In the sense that we
can't always have an explicit formula, often, the function $f$ is computed by
another program that is able to compute $f(x)$ and $\nabla f(x)$ for a given
value of $x$. In order to have an efficient algorithm, we want to avoid any
unnecessary computation of $f(x)$ and its derivatives. We will denote the values
taken by $x$ along the iterations as $x_0$, $x_1$, $x_2$,\ldots $x_i$. And
$f(x_i)$ is denoted $f_i$. 

Since our knowledge of the objective function is only partial, it would not be
possible to guarantee that a point $x^*$ is a global solution: 
\begin{equation}
  \forall x \in \mathcal{S}, f(x^*) \leq f(x)
\end{equation}
Though, we can find a local minimizer of $f$: 
\begin{equation}
  \text{There exist a neighborhood } \mathcal{N}\text{ of }x^*\text{ such that
  }\forall x\in \mathcal{N}, \ f(x^*) \leq f(x) 
\end{equation}
That is the kind of solution that we are looking for and that our algorithms will
find.

Under the assumption that the objective function is smooth and sufficiently
continuous ($\mathcal{C}_2$), then we have the following sufficient conditions
for the optimality of $x^*$ as presented in \cite{nocedal:book:2006}:

\begin{theorem}
  If $\nabla^2f$ is continuous in an open-neighborhood of $x^*$ and that $\nabla
  f(x^*)=0$ and $\nabla^2 f(x^*)$ is positive definite. Then $x^*$ is a strict
  local minimizer of $f$.
  \label{optimalityTheorem}
\end{theorem}

\subsection{globalization methods}
During the resolution of an optimization problem, the algorithm will generate a
sequence of iterates $x_k$ starting from the initial iterate $x_0$ (Which is
usually provided by the user). During the step $k$ of the optimization process,
the solver, the current point is $x_k$ and we try to find $x_{k+1}$ such that
$f(x_{k+1}) < f(x_k)$. There are several strategies to do that but we will only focus
on 2 of them that are the most popular, the line-search and trust-region
methods.

\subsubsection{The Line-Search Strategy}
In the Line-Search strategy, given a point $x_k$, a descent direction $p_k$ from this
point is chosen and then a length of step is calculated to minimize the
following 1-dimentional problem:
\begin{align}
  \min_{\bf \alpha \in \mathbb{R}^+} & \quad \mbox{\emph{f}}(x_k + \alpha.p_k)
\label{eq:lineSearch}
\end{align}

Once the best value of $\alpha$ has been found, the next iterate is computed:
$x_{k+1} = x_{k} + \alpha p_k$ and the same process is repeated until a
satisfying solution is found. 

It is not always necessary to find the optimal value of $\alpha$, especially if
that is expensive. Indeed, $\alpha$ is only used to calculate the next iterate,
from which another $p_k$ and $\alpha$ will be calculated. And in the end, the
imprecision on the computation of $\alpha$ will be errased in the other steps of
the resolution.

There are several ways to choose a descent direction from an iterate $x_k$. The
most obvious one is probably the steepest descent direction $-\nabla f_k$. This
method provides the direction along which f decreases most rapidly and only
requires the evaluation of the first derivative of $f$, but that method can
become extremely slow on complicated problems. Another popular approach is the
Newton method, in which the objective function is approximated to the second
order

\begin{equation}
  f(x_k+p) = f_k + p^T\nabla f_k + p^T\nabla^2f_k p
\end{equation}

Then the chosen descent direction is the optimum of that approximated function,
the Newton direction.
\begin{equation}
  p^N_k = -(\nabla^2 f_k)^{-1} \nabla f_k
\end{equation}
The choice of this descent direction implies that the $\nabla^2f_k$ is positive
definite, in which case an adaptation of the definition of $p_k$ is required. Or
an approximation $B_k$ of $\nabla^2f_k$ that guaranties definite positiveness can be
used. For example, the symmetric-rank-one(SR1) formula and the BFGS(Broyden,
Fletcher, Goldfarb, Shanno) formula. Then the step becomes 

\begin{equation}
  p_k = -B_k^{-1}\nabla f_k
\end{equation}

\subsubsection{The Trust-Region Strategy}
The Trust-Region Strategy works in an opposite way than the line-search one in
the sense that during a line-search step, a direction is chosen, and based on
that direction, a step-length is chosen. Whereas with a Trust-Region approach, a
maximum step-length is chosen, and based on it, the descent direction and lenght
are chosen.
The principle of the trust region is that along the optimization process, a
model of the problem is constructed and enriched at every step and at each step,
the next iterate is the optimum of the model, with the constraint that the step
to get there lies inside the trust-region. For example, let us considere that
the trust region is a sphere of center $x_k$ and radius $\rho_k$, then the
constraint on $p_k$ is $\|p_k\| \leq \rho_k$. A usual model to take for the
objective function is the quadratic model with approximated Hessian
\begin{equation}
  m_k(p) = f(x_k+p) = f_k + p^T\nabla f_k + p^TB_k p
\end{equation}
And the optimization problem to solve at each step of the optimization is

\begin{align}
  \min_{p} & \quad f_k + p^T\nabla f_k + p^TB_k p \nonumber\\
\text{s.t.}&
\quad \|p\| \leq \rho_k
\label{eq:trustRegion}
\end{align}

Once the solution $p_k$ to this quadratic problem is found, its quality is
estimated by evaluating the value of $f(x_k+p_k)$. If the actual decrease of $f$
is satisfying (compared to the decrease predicted by the model) then the step is
accepted and the size of the trust-region can be increased. Otherwise, the step
is refused, the trust-region radius is reduced and a new step from $x_k$ is
computed on that smaller trust region.

\section{Constrained Optimization}

Solving a constraint optimization problem consists in minimizing a function while being subject to constraints on the variables.

A general formulation for such problems is:

\begin{equation}
  \label{formulation_NLCP}
  \min_{x\in\mathbb{R}^n}{f(x)} \text{ subject to }
  \left\{
  \begin{array}{l}
    c_i(x) = 0,\ \forall i\in{E}\\
    c_i(x) \geq 0,\ \forall i\in{I}\\
  \end{array}
  \right.
\end{equation}


Where $f$ and $c_i$ are real valued functions on a subset of $\mathbb{R}^n$.
$f$ is the objective function. And $c_i$ are the constraint functions.
${I}$ and ${E}$ are sets of index such that $c_i,\ i\in{E}$ are the equality constraints and $c_i,\ i\in{I}$ are the inequality constraints.

We define the feasible set $\Omega$ that contains all the feasible points (points satisfying the constraints) of \ref{formulation_NLCP}.

\begin{equation}
  \Omega = \left\{ x\in \mathbb{R}^n:\ \forall i\in {E},\ c_i=0,\ \forall i\in{I},\ c_i \geq0\right\}
\end{equation}

The formulation \ref{formulation_NLCP} can then be rewritten as:
\begin{equation}
  \label{formulation_NLCP_compact}
  \min_{x\in\Omega}{f(x)}
\end{equation}

In the general case, the problem \ref{formulation_NLCP_compact} has multiple local solutions.
And finding the global minimizer of $f$ in $\Omega$ is a difficult problem that we do not treat in this thesis.
Our goal is to find a local minimizer $x^*$ of $f$ in $\Omega$.

\subsection{Optimality conditions}

\paragraph{First-Order Optimality Conditions}

The First Order Optimality Condition (Karush-Kuhn-Tucker Condition) is a necessary condition verified by all solutions of problem \ref{formulation_NLCP}.

We introduce the Lagrangian function of \ref{formulation_NLCP}:
\begin{equation}
  \mathcal{L}(x,\lambda) = f(x) + \sum_{i\in E\cup I}\lambda_i c_i(x)
\end{equation}

Any given constraint $c_i$ is said to be active at $x$ if $c_i(x)=0$.
In particular, for any feasible point $x$, all equality constraints $c_i,\ i\in E$ are active. 
An inequality constraint $c_i,\ i\in I$ is inactive if $c_i(x)>0$ and active if $c_i(x) = 0$

\begin{definition}  
  \label{active_set}
  The active set $\mathit{A}(x)$ at a feasible point $x$ is the set of all the indexes of active constraint.
  \begin{equation}
    \mathit{A}(x)=E\cup\{i\in I: c_i(x) = 0\}
  \end{equation}
\end{definition}

Most optimization algorithms make the assumption that at the solution, the constraints satisfy the Linear Independence Constraints Qualification (LICQ)

\begin{definition}
  Given a point $x$ and an active set $\mathit{A}(x)$, the LICQ holds if the set of active constraint gradient $\{\nabla c_i(x),\ i\in \mathit{A}(x)\}$ is linearly independent
\end{definition}

In particular, if any gradient of a constraint is null, then the LICQ does not hold. 
The LICQ should be taken into account during the formulation of a problem.

\begin{theorem}(First-Order Necessary Conditions)\\
  \label{KKT_conditions}
  Suppose that $x^*$ is a local solution of \ref{formulation_NLCP}, that the functions $f$ and $c_i$ are continuously differentiable, and that the LICQ holds at $x^*$.
  Then there is a Lagrange multiplier vector $\lambda^*$, with components $\lambda_i^*$, $i\in E\cup I$, such that the following conditions are satisfied at $(x^*,\lambda^*)$
  \begin{equation}
  \begin{array}{ll}
    \nabla_x\mathcal{L}(x^*,\lambda^*) = 0 &, \\
    c_i(x^*) = 0 &,\ \forall i\in E\\
    c_i(x^*) \geq 0 &,\ \forall i\in I\\
    \lambda_i^* \geq 0 &,\ \forall i\in I\\
    \lambda_i^* c_i(x^*)=0 &,\ \forall i \in E\cup I\\
  \end{array}
  \end{equation}
\end{theorem}

In many cases, the main goal of an optimization algorithm is to find a point that satisfies the KKT conditions.

\paragraph{Second-Order Optimality Conditions}

\begin{definition}
  Given a feasible point $x$, and the active set $\mathit{A}(x)$, the  set of linearized feasible directions $F(x)$ is:
  \begin{equation}
    F(x)=\left\{d|
        \begin{array}{ll}
          d^T\nabla c_i(x) = 0&,\ \forall i\in E \\
          d^T\nabla c_i(x) \geq 0&,\ \forall i\in \mathit{A}(x)\cap I \\
        \end{array}
    \right\}
  \end{equation}
\end{definition}

And at a KKT solution $(x^*,\lambda^*)$ the critical cone $C(x^*, \lambda^*)$ is:

\begin{equation}
  C(x^*,\lambda^*) = \{w\in F(x^*)|\nabla c_i(x^*)^Tw=0, \forall i\in\mathit{A}(x^*)\cap I \text{ with } \lambda_i^*>0\}
\end{equation}

THe critical cone contains the linearly feasible directions for which it ix not clear from first derivative information alone if $f$ will increase or decrease.

Once the KKT condition is reached for a point $(x^*, \lambda^*)$, a direction $w$ of $F(x^*)$ for which $w^T\nabla f(x^*)=0$.
For such directions, it is impossible to tell from first derivative information only, if an increment in this direction will have a positive effect on the problem resolution.
Thus it is possible that the KKT constaint is not enough to determine if a point is a solution or just a stationary point.

The Second-Order Sufficient Conditions allow to discriminate local solutions from stationary points:

\begin{theorem}
  Suppose that for some feasible point $x^*\in \mathbb{R}^n$ there is a Lagrange multiplier vector $\lambda^*$ such that the KKT conditions are satisfied. Suppose also that
  \begin{equation}
    w^T\nabla_{xx}^2\mathcal{L}(x^*,\lambda^*)w>0,\ \forall w\in C(x^*,\lambda^*),\ w\neq 0
  \end{equation}
  Then $x^*$ is a strict local solution for \ref{formulation_NLCP}
\end{theorem}

\section{Resolution of a Non Linear Constrained Optimization}

In the previous section, we presented some theoretical tools to describe the solution of an NLCP that only use the derivative terms of the problem's functions of order one and two.
Here we present some methods for acually solving such problems.
There exists many different algorithms to solve an NLCP.
But all have in common to be iterative processes.
In which we start from an initial guess $x_k$ for $x^*$.
From the first and (sometimes) second-order informations on the functions $f$ and $c_i$ that constitute the problem \ref{formulation_NLCP}, we find a suitable increment $z$ such tha $x_k+z$ is closer to the solution than $x_k$.
And we iterate that opertation until a point $x_k$ sufficiently close to a solution is found.
By sufficiently close to a solution we mean that this point satisfies the optimality conditions with a good enough precision.

The resolutions algorithms differ mostly by the method chosen to generate a satisfactory increment $z$. We will list here some of the most usual methods:

\paragraph {The penalty method} combines the cost and constraints function in a penalty function that, as its name indicates, penalizes the violation of constraints, without completely proscribing it.
The penalty function can be written as follows, using the notation $[y]^- = \max\{0, -y\}$:

\begin{equation}
  p(\mu, x) = f(x) + \mu \sum_{i\in E}|c_i(x)| + \mu \sum_{i\in I} [c_i(x)]^-
\end{equation}

With this approach, at each step an unconstrained optimization problem is solved.
The minimum of $p(\mu,x)$ is found and varies with the penalty parameter $\mu$.
By increasing $\mu$ to $\infty$, we penalize the violation of the constraints with increasing severity until reaching the solution $x^*$.

\paragraph{The interior point method} generates steps by solving a relaxed constrained problem where slack variables are introduced to relax inequality constraints:

\begin{equation}
  \min_{x,s}{ f(x)- \mu\sum_{i\in I} \log(s_i)}
  \text{ subject to }
  \left\{
    \begin{array}{l}
     c_i = 0,\ i\in E\\
     c_i - s_i = 0,\ i\in I\\
     s_i \geq 0,\ i\in I
  \end{array}
  \right.
\end{equation}

\paragraph{The Sequential Quadratic Programming (SQP)} is a method in which, at each iterate $(x_k, \lambda_k)$, the increment $z$ is found by solving a Quadratic Programming (QP).
The QP is an approximation of the KKT conditions \ref{KKT_conditions} of the actual problem \ref{formulation_NLCP} to the first order:

\begin{equation}
  \label{approx_QP}
  \begin{array}{ll}
    \min_{z}{} & \frac{1}{2}z^T\nabla_{xx}^2\mathcal{L}(x_k, \lambda_k)z + \nabla f(x_k)^Tz \\
    \text{subject to } & \nabla c_i(x_k)^Tz+c_i(x_k)=0,\ i\in E \\
                       & \nabla c_i(x_k)^Tz+c_i(x_k)\geq 0,\ i\in I
  \end{array}
\end{equation}

Using that method as is can be problematic as the length of the iterate needs not be bounded.
Thus it is possible to generate a very large step that satisfies the QP.
Though the QP only approximates the original problem locally.
So taking a too big step can get us further from the solution than we were previously.
To paliate to that issue, mainly two methods are used:
\begin{itemize}
  \item The line-search method: The solution $z$ of \ref{approx_QP} is viewed as a direction and we search a parameter $\alpha\in [0;1]$ so that the next iterate $x_k + \alpha z$ is optimal for the original problem.
  \item The trust-region method adds a set of bound constraints to the QP \ref{approx_QP}. So that the length of the step is limited by the trust-region. The size of the trust region is modified along the iterations based on the estimated quality of the QP approximation.
\end{itemize}

\section{Sequential Quadratic Programming}
\label{sec:sequential_quadratic_programming}

The Sequential Quadratic Programming method is one of the most effective method to solve small and large scale non-linear constrained optimization problems.
It has the upper hand on other methods mostly when solving problem with highly non-linear constraints.

\subsection{Principle}
\label{sub:principle}

As we stated before, the main idea behind the SQP approach it to solve a subproblem that approximate linearly to the first order the KKT conditions.
For simplicity, we considere a problem without inequality constraints.
We denote $C$ the vector of equality constraints.
We denote $z_x$ and $z_\lambda$ the increments on $x$ and $\lambda$ respectively.
\begin{equation}
  \begin{array}{l}
    x_{k+1} = x_k + z_x\\
    \lambda_{k+1} = \lambda_k+z_\lambda\\
  \end{array}
\end{equation}

The problem is the following:
\begin{equation}
  \begin{array}{l}
    \min_{x\in\mathbb{R}^n}{f(x)} \\
    \text{ subject to } C(x) = 0
  \end{array}
\end{equation}

Its KKT conditions are:
\begin{equation}
  \label{KKT_equ}
  \left\{
\begin{array}{ll}
  \nabla_x\mathcal{L}(x^*,\lambda^*) = 0\\
  C(x^*) = 0\\
\end{array}
\right.
\end{equation}

We denote with a subscript $y^k$ the value of the quantity $y$ evaluated at point $(x_k, \lambda_k)$.

The first order linearization of \ref{KKT_equ} gives:

\begin{equation}
  \label{KKT_1st_order}
  \begin{array}{l}

  \left\{
\begin{array}{l}
  \nabla_{xx}^2\mathcal{L}_k z_x + \nabla_{x\lambda}^2\mathcal{L}_k z_\lambda + \nabla_x\mathcal{L}_k  = 0\\
  \nabla_x C_k z_x + C_k = 0\\
\end{array}
\right. \\
\text{which gives:}\\
  \left\{
\begin{array}{l}
  \nabla_{xx}^2\mathcal{L}_k z_x + \nabla_{x}C_k (\lambda_k + z_\lambda) = - \nabla_{x}f_k\\
  \nabla_x C_k z_x = - C_k \\
\end{array}
\right. \\
\text{Or in matrix form:}\\
  \begin{pmatrix}
      \nabla_{xx}^2\mathcal{L}_k & \nabla_x C_k\\
      \nabla_x C_k & 0\\
  \end{pmatrix}
  \begin{pmatrix}
      z_x\\
      \lambda_{k+1}\\
  \end{pmatrix}
  =
  \begin{pmatrix}
      - \nabla_{x}f_k\\
      - C_k\\
  \end{pmatrix}
  \end{array}
\end{equation}

Solving this problem actually is equivalent to solving a QP problem of the following form:

\begin{equation}
  \begin{array}{ll}
    \min_{z_x} &f_k + \nabla_x f_k ^T z_x + \frac{1}{2} z_x^T\nabla_{xx}^2\mathcal{L}_k z_x \\
    \text{s.t.} & \nabla_x C_k^T z_x + C_k = 0\\
  \end{array}
\end{equation}

The unique solution of this problem satisfies the following matrix equality:

\begin{equation}
  \begin{pmatrix}
      \nabla_{xx}^2\mathcal{L}_k & -\nabla_x C_k\\
      \nabla_x C_k & 0\\
  \end{pmatrix}
  \begin{pmatrix}
      z_x\\
      l_k\\
  \end{pmatrix}
  =
  \begin{pmatrix}
      - \nabla_{x}f_k\\
      - C_k\\
  \end{pmatrix}
\end{equation}

Note that this problem has a solution if the following assumptions hold:
\begin{itemize}
  \item The constraint Jacobian $\nabla_x C_k$ has full rank.
  \item The matrix $\nabla_{xx}^2\mathcal{L}_k$ is positive definite on the tangent space of the constraints:\\ $d^T\nabla_{xx}^2\mathcal{L}_k d \geq 0,\ \forall d\neq 0$ such that ${\nabla_x C_k}^T d = 0$.
\end{itemize}

Therefore, the problem raised by the linearization of the KKT conditions to the first order can be solved as a QP problem.
Denoting the solution of the QP problem $(z_x, l_k)$, the solution to \ref{KKT_1st_order} is given by

\begin{equation}
  \begin{pmatrix}
      z_x\\
      \lambda_{k+1}\\
  \end{pmatrix}
  \leftarrow
  \begin{pmatrix}
      z_x\\
      -l_k\\
  \end{pmatrix}
\end{equation}

This development can easily be extended to treat the case of problems constrained with inequality constraints:

\begin{equation}
  \min_{x\in\mathbb{R}^n}{f(x)} \text{ subject to }
  \left\{
  \begin{array}{l}
    c_i(x) = 0,\ \forall i\in{E}\\
    c_i(x) \geq 0,\ \forall i\in{I}\\
  \end{array}
  \right.
\end{equation}

By solving the following IQP system at each iteration:

\begin{equation}
  \label{IQP}
  \begin{array}{ll}
    \min_{z_x} &f_k + \nabla_x f_k ^T z_x + \frac{1}{2} z_x^T\nabla_{xx}^2\mathcal{L}_k z_x \\
    \text{s.t.} & \nabla_x {c_i}_k z_x + {c_i}_k = 0 ,\ \forall i\in E\\
                & \nabla_x {c_i}_k z_x + {c_i}_k \geq 0 ,\ \forall i\in I\\
  \end{array}
\end{equation}

The main difficulty in solving that QP \ref{IQP} comes from finding the optimal active set at each step $\mathit{A}_k$.
To find it, the QP will start from a random active set $\mathit{A}_0$ ignore the non-active constraints and try to solve the QP with the remaining active constraint, then if the solution is not satisfactory, it will change the active set and iterate over this process until the optimal active set and solution is found for the QP \ref{IQP}.
Once the SQP gets close to the solution, the set of active constraints should not change much from one iterate $x_k$ to the next one.
It is often very useful to initialize the IQP's active set with the optimal active set of the previous iterate: $\mathit{A}_0(x_{k+1})\leftarrow\mathit{A}^*(x_k)$.
That method is called the \textit{warm-start}.

The SQP approach to solving the problem \ref{formulation_NLCP} as presented above leans on the assumption that at each step, the QP generated is feasible,
which requires that the Hessian $\nabla_{xx}^2\mathcal{L}$ is positive definite on the tangent space of the active constraints.
That property is guarantied to hold near the solution, but not when staring from a remote point on a non-convex problem.
To quope with that issue, several methods exist.
In the next few section, we will present the globalisation methods that are the Line Search and Trust Region, they make sure that the QP subproblem is always feasible.
Then we'll present some methods used to choose whether to accept of reject an iterate with the merit function and filter methods.
And finally we will discuss some Hessian approximation methods.

\subsection{Globalization methods}
\label{sub:globalization_methods}




\section{Conclusion}
This section gives a general introduction to Non-linear constrained optimization without regards for robotics or optimization on manifolds

